[
  {
    "doi": "10.1101/004184",
    "title": "Flexible methods for estimating genetic distances from nucleotide data",
    "authors": "Simon Joly;David J Bryant;Peter J Lockhart;",
    "author_corresponding": "Simon  Joly",
    "author_corresponding_institution": "Montreal Botanical Garden",
    "date": "2014-11-28",
    "version": "2",
    "type": "New Results",
    "license": "cc_by",
    "category": "Evolutionary Biology ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/004184.source.xml",
    "abstract": "O_LIWith the increasing use of massively parallel sequencing approaches in evolutionary biology, the need for fast and accurate methods suitable to investigate genetic structure and evolutionary history are more important than ever. We propose new distance measures for estimating genetic distances between individuals when allelic variation, gene dosage and recombination could compromise standard approaches.\\nC_LIO_LIWe present four distance measures based on single nucleotide polymorphisms (SNP) and evaluate them against previously published measures using coalescent-based simulations. Simulations were used to test (i) whether the measures give unbiased and accurate distance estimates, (ii) if they can accurately identify the genomic mixture of hybrid individuals and (iii) if they give precise (low variance) estimates.\\nC_LIO_LIThe results showed that the SNP-based GENPOFAD distance we propose appears to work well in the widest circumstances. It was the most accurate method for estimating genetic distances and is also relatively good at estimating the genomic mixture of hybrid individuals.\\nC_LIO_LIOur simulations provide benchmarks to compare the performance of different distance measures in specific situations.\\nC_LI",
    "published": "10.1111/2041-210X.12343",
    "server": "biorxiv"
  },
  {
    "doi": "10.1101/008979",
    "title": "Evolving mutation rate advances invasion speed of sexual species",
    "authors": "Marleen M. P. Cobben;Alexander Kubisch;",
    "author_corresponding": "Marleen M. P. Cobben",
    "author_corresponding_institution": "Netherlands Institute of Ecology",
    "date": "2014-11-28",
    "version": "2",
    "type": "New Results",
    "license": "cc_by_nc_nd",
    "category": "Evolutionary Biology ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/008979.source.xml",
    "abstract": "Many species are shifting their ranges in response to global climate change. The evolution of dispersal during range expansion increases invasion speed, provided that a species can adapt sufficiently fast to novel local conditions. Mutation rates can evolve too, under conditions that favor an increased rate of adaptation. However, evolution at the mutator gene has thus far been deemed of minor importance in sexual populations due to its dependence on genetic hitchhiking with a beneficial mutation at a gene under selection, and thus its sensitivity to recombination. Here we use an individual-based model to show that the mutator gene and the gene under selection can be effectively linked at the population level during invasion. This causes the evolutionary increase of mutation rates in sexual populations, even if they are not linked at the individual level. The observed evolution of mutation rate is adaptive and clearly advances range expansion both through its effect on the evolution of dispersal rate, and the evolution of local adaptation. In addition, we observe the evolution of mutation rates in a spatially stable population under strong directional selection, but not when we add variance to the mean selection pressure. By this we extend the existing theory on the evolution of mutation rates, which is generally thought to be limited to asexual populations, with possibly far-reaching consequences concerning invasiveness and the rate at which species can adapt to novel environmental conditions as experienced under global climate change.",
    "published": "10.1186/s12862-017-0998-8",
    "server": "biorxiv"
  },
  {
    "doi": "10.1101/011882",
    "title": "Reveel: large-scale population genotyping using low-coverage sequencing data",
    "authors": "Lin Huang;Bo Wang;Ruitang Chen;Sivan Bercovici;Serafim Batzoglou;",
    "author_corresponding": "Serafim  Batzoglou",
    "author_corresponding_institution": "Stanford University",
    "date": "2014-11-28",
    "version": "2",
    "type": "New Results",
    "license": "cc_by_nc_nd",
    "category": "Bioinformatics ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/011882.source.xml",
    "abstract": "Population low-coverage whole-genome sequencing is rapidly emerging as a prominent approach for discovering genomic variation and genotyping a cohort. This approach combines substantially lower cost than full-coverage sequencing with whole-genome discovery of low-allele-frequency variants, to an extent that is not possible with array genotyping or exome sequencing. However, a challenging computational problem arises when attempting to discover variants and genotype the entire cohort. Variant discovery and genotyping are relatively straightforward on a single individual that has been sequenced at high coverage, because the inference decomposes into the independent genotyping of each genomic position for which a sufficient number of confidently mapped reads are available. However, in cases where low-coverage population data are given, the joint inference requires leveraging the complex linkage disequilibrium patterns in the cohort to compensate for sparse and missing data in each individual. The potentially massive computation time for such inference, as well as the missing data that confound low-frequency allele discovery, need to be overcome for this approach to become practical. Here, we present Reveel, a novel method for single nucleotide variant calling and genotyping of large cohorts that have been sequenced at low coverage. Reveel introduces a novel technique for leveraging linkage disequilibrium that deviates from previous Markov-based models. We evaluate Reveels performance through extensive simulations as well as real data from the 1000 Genomes Project, and show that it achieves higher accuracy in low-frequency allele discovery and substantially lower computation cost than previous state-of-the-art methods.",
    "published": "10.1093/bioinformatics/btv530",
    "server": "biorxiv"
  },
  {
    "doi": "10.1101/011940",
    "title": "Alignment by numbers: sequence assembly using compressed numerical representations",
    "authors": "Avraam Tapinos;Bede Constantinides;Douglas B Kell;David L Robertson;",
    "author_corresponding": "David L Robertson",
    "author_corresponding_institution": "University of Manchester",
    "date": "2014-11-28",
    "version": "1",
    "type": "New Results",
    "license": "cc_by_nc_nd",
    "category": "Bioinformatics ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/011940.source.xml",
    "abstract": "MotivationDNA sequencing instruments are enabling genomic analyses of unprecedented scope and scale, widening the gap between our abilities to generate and interpret sequence data. Established methods for computational sequence analysis generally use nucleotide-level resolution of sequences, and while such approaches can be very accurate, increasingly ambitious and data-intensive analyses are rendering them impractical for applications such as genome and metagenome assembly. Comparable analytical challenges are encountered in other data-intensive fields involving sequential data, such as signal processing, in which dimensionality reduction methods are routinely used to reduce the computational burden of analyses. We therefore seek to address the question of whether it is possible to improve the efficiency of sequence alignment by applying dimensionality reduction methods to numerically represented nucleotide sequences.\\n\\nResultsTo explore the applicability of signal transformation and dimensionality reduction methods to sequence assembly, we implemented a short read aligner and evaluated its performance against simulated high diversity viral sequences alongside four existing aligners. Using our sequence transformation and feature selection approach, alignment time was reduced by up to 14-fold compared to uncompressed sequences and without reducing alignment accuracy. Despite using highly compressed sequence transformations, our implementation yielded alignments of similar overall accuracy to existing aligners, outperforming all other tools tested at high levels of sequence variation. Our approach was also applied to the de novo assembly of a simulated diverse viral population. Our results demonstrate that full sequence resolution is not a prerequisite of accurate sequence alignment and that analytical performance can be retained and even enhanced through appropriate dimensionality reduction of sequences.",
    "published": "NA",
    "server": "biorxiv"
  },
  {
    "doi": "10.1101/011940",
    "title": "Alignment by numbers: sequence assembly using compressed numerical representations",
    "authors": "Avraam Tapinos;Bede Constantinides;Douglas B Kell;David L Robertson;",
    "author_corresponding": "David L Robertson",
    "author_corresponding_institution": "University of Manchester",
    "date": "2014-11-28",
    "version": "2",
    "type": "New Results",
    "license": "cc_by_nc_nd",
    "category": "Bioinformatics ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/011940.source.xml",
    "abstract": "MotivationDNA sequencing instruments are enabling genomic analyses of unprecedented scope and scale, widening the gap between our abilities to generate and interpret sequence data. Established methods for computational sequence analysis generally use nucleotide-level resolution of sequences, and while such approaches can be very accurate, increasingly ambitious and data-intensive analyses are rendering them impractical for applications such as genome and metagenome assembly. Comparable analytical challenges are encountered in other data-intensive fields involving sequential data, such as signal processing, in which dimensionality reduction methods are routinely used to reduce the computational burden of analyses. We therefore seek to address the question of whether it is possible to improve the efficiency of sequence alignment by applying dimensionality reduction methods to numerically represented nucleotide sequences.\\n\\nResultsTo explore the applicability of signal transformation and dimensionality reduction methods to sequence assembly, we implemented a short read aligner and evaluated its performance against simulated high diversity viral sequences alongside four existing aligners. Using our sequence transformation and feature selection approach, alignment time was reduced by up to 14-fold compared to uncompressed sequences and without reducing alignment accuracy. Despite using highly compressed sequence transformations, our implementation yielded alignments of similar overall accuracy to existing aligners, outperforming all other tools tested at high levels of sequence variation. Our approach was also applied to the de novo assembly of a simulated diverse viral population. Our results demonstrate that full sequence resolution is not a prerequisite of accurate sequence alignment and that analytical performance can be retained and even enhanced through appropriate dimensionality reduction of sequences.",
    "published": "NA",
    "server": "biorxiv"
  },
  {
    "doi": "10.1101/011890",
    "title": "Generalised empirical Bayesian methods for discovery of differential data in high-throughput biology",
    "authors": "Thomas J Hardcastle;",
    "author_corresponding": "Thomas J Hardcastle",
    "author_corresponding_institution": "University of Cambridge",
    "date": "2014-11-28",
    "version": "1",
    "type": "New Results",
    "license": "cc_by_nc_nd",
    "category": "Bioinformatics ",
    "jatsxml": "https://www.biorxiv.org/content/early/2014/11/28/011890.source.xml",
    "abstract": "MotivationHigh-throughput data are now commonplace in biological research. Rapidly changing technologies and application mean that novel methods for detecting differential behaviour that account for a  large P, small n setting are required at an increasing rate. The development of such methods is, in general, being done on an ad hoc basis, requiring further development cycles and a lack of standardization between analyses.\\n\\nResultsWe present here a generalised method for identifying differential behaviour within high-throughput biological data through empirical Bayesian methods. This approach is based on our baySeq algorithm for identification of differential expression in RNA-seq data based on a negative binomial distribution, and in paired data based on a beta-binomial distribution. Here we show how the same empirical Bayesian approach can be applied to any parametric distribution, removing the need for lengthy development of novel methods for differently distributed data. Comparisons with existing methods developed to address specific problems in high-throughput biological data show that these generic methods can achieve equivalent or better performance. A number of enhancements to the basic algorithm are also presented to increase flexibility and reduce computational costs.\\n\\nAvailabilityThe methods are implemented in the R baySeq (v2) package, available on Bioconductor http://www.bioconductor.org/packages/release/bioc/html/baySeq.html.\\n\\nContacttjh48@cam.ac.uk",
    "published": "10.1093/bioinformatics/btv569",
    "server": "biorxiv"
  }
]