{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Continue Scraping\n",
    "\n",
    "---\n",
    "\n",
    "This current notebook continues scraping from where it was previously left off. It is meant to be run after the initial scraping notebook, `initial-scraper.ipynb`, has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import calendar\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_available_date():\n",
    "    '''\n",
    "    Function to automatically obtain the last available date of the data and return it\n",
    "        @param None: No parameters, as this automatically obtains the last available date\n",
    "        @return last_date: The last available date of the data\n",
    "    '''\n",
    "    max_year = max([int(year_folder) for year_folder in os.listdir('./full-dict')]) ## Get last year\n",
    "    max_month = max([int(month_folder) for month_folder in os.listdir(f'./full-dict/{max_year}')]) ## Get last month\n",
    "    last_date = [x[:-5] for x in os.listdir(f'./full-dict/{max_year}/{max_month}/')][-1] ## get last date from the file\n",
    "\n",
    "    year, month, day = map(int, last_date.split('-')) # Assuming the date format is YYYY-MM-DD, split the string to get year, month, and day\n",
    "\n",
    "    return date(year, month, day) # Return a datetime.date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_return_dict(articles, cur_date):\n",
    "    '''\n",
    "    This function processes all of the articles associated with a specific date, it checks:\n",
    "        1) Whether the current DOI has ALREADY been processed, if so updates the saved JATSXML link in associated JSON\n",
    "        2) Whether the key for the license associated with the current article exists, if so, a new key is then created and added to the JSON value for that key in particular\n",
    "            @param article: The article JSON object to be processed\n",
    "            @return: None\n",
    "    '''\n",
    "    cur_date = str(cur_date)\n",
    "    articles = articles['collection']\n",
    "    for article in articles:\n",
    "        ## Tracks the number of license type count from the return_dict \"license\" object\n",
    "        return_dict['license'][article['license']] = return_dict['license'].get(article['license'], 0) + 1\n",
    "\n",
    "        ## Adds/UPDATES the most recent DOI submission from return_dict\n",
    "        return_dict['content'][article['doi']] = {\n",
    "            'date': article['date'],\n",
    "            'license': article['license'],\n",
    "            'jatsxml': article['jatsxml']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_full_dict_by_date(articles, cur_date):\n",
    "    '''\n",
    "    This function adds to a dict object where the keys are the dates in string format and the values are a list of DOIs for that date\n",
    "        @param articles: All articles in current date to extract information from\n",
    "        @param cur_date: The current date to process in YYYY-mm-dd format\n",
    "        @return: None\n",
    "    '''\n",
    "    cur_date = str(cur_date)\n",
    "    articles = articles['collection']\n",
    "    \n",
    "    full_dict_by_date[cur_date] = [{'doi': item['doi'], 'license': item['license']} for item in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dated_subfolders(date):\n",
    "    '''\n",
    "    Function to create subfolders for each of the day, month, year in the current date, specifically to store metadata from \"full_dict_by_date\"\n",
    "        @param date: The current date in YYYY-mm-dd format\n",
    "        @return: The full output path for the folder representing the current date\n",
    "    '''\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "\n",
    "    dict_main_folder = Path('./full-dict')\n",
    "\n",
    "    dict_year_folder = dict_main_folder.joinpath(str(year))\n",
    "    dict_year_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    dict_month_folder = dict_year_folder.joinpath(str(month))\n",
    "    dict_month_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return dict_month_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dates(dates, num_splits):\n",
    "    '''\n",
    "    Function that takes in a list of dates (or anything, really) and splits those dates into num_splits number of splits\n",
    "        @param dates: A list of dates (or anything really)\n",
    "        @param num_splits: The number of sub-lists to split the original list into\n",
    "    '''\n",
    "    first_last_dates_fx = lambda lis: [[li[0], li[-1]] for li in lis] ## Essentially extracts the first and last elements of the split list of dates\n",
    "    return list(first_last_dates_fx(np.array_split(dates, num_splits))) ## Returns the first and last dates of each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_folder(split_num):\n",
    "    '''\n",
    "    Function that creates a folder for the specific split applicable to the current operation\n",
    "        @param split_num: The split number to obtain the associated folder for\n",
    "        @return: The path to the folder for the split number\n",
    "    '''\n",
    "    split_main_folder = Path('./amend_dicts/')\n",
    "    split_main_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    split_folder = split_main_folder.joinpath(str(split_num))\n",
    "    split_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return split_folder ## Not needed but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bioRxiv(start_date=lambda _: date(2013, 1, 1),\n",
    "                    end_date=lambda _: date.today(),\n",
    "                    load_dicts=False,\n",
    "                    delay=5,\n",
    "                    **kwargs):\n",
    "    '''\n",
    "    Function to process the data (loading in if necessary) for a period of time defined by:\n",
    "        @param start: Beginning date in the format YYYY-mm-dd\n",
    "        @param end: End date in the format YYY-mm-dd\n",
    "        @param save_every: Saves (overrites) the last dict every n number of iterations through the data\n",
    "        @param load_dicts: Loads in the dictionary (JSON) objects associated with \"full_dict_by_date\" and \"return_dict\"\n",
    "    '''\n",
    "    global full_dict_by_date\n",
    "    full_dict_by_date = {}\n",
    "\n",
    "    global return_dict\n",
    "    return_dict = {\n",
    "        'license': {},\n",
    "        'content': {}\n",
    "    }\n",
    "\n",
    "    get_split_folder(SELECTED_NUMBER) ## Creates the folder for the current split number\n",
    "\n",
    "    if load_dicts:\n",
    "        with open(f'{SELECTED_NUMBER if SELECTED_NUMBER else \".\"}/full_dict_by_date.json', 'r') as f:\n",
    "            full_dict_by_date = json.load(f)\n",
    "        with open(f'{SELECTED_NUMBER if SELECTED_NUMBER else \".\"}/return_dict.json', 'r') as f:\n",
    "            return_dict = json.load(f)\n",
    "\n",
    "        start_date = date(full_dict_by_date.keys().sort()[-1]) + timedelta(days=1) ## Gets the last available date and adds 1 IF not the first date, otherwise starts at Jan 1, 2013\n",
    "\n",
    "    dates = [start_date + timedelta(days=x) for x in range((end_date  - start_date).days + 1)] ## Gets a list of available dates\n",
    "\n",
    "    for cur_date in dates:\n",
    "\n",
    "        flag = True ## Flag of whether or not to continue the loop to get more pages\n",
    "        num_skip = 0 ## Flag to skip as per API instructions\n",
    "        while flag:\n",
    "            cur_date_dir = get_dated_subfolders(cur_date)\n",
    "\n",
    "            url = f'https://api.biorxiv.org/details/biorxiv/{str(cur_date)}/{str(cur_date)}/{num_skip}'\n",
    "\n",
    "            response = requests.get(url) ## Processes request and waits for response / articles in JSON format\n",
    "            response.raise_for_status()\n",
    "            articles = response.json()\n",
    "\n",
    "            if articles['messages'][0]['status'] != 'ok': ## Continues onto the next date if posts for the current date are unavailable\n",
    "                flag = False\n",
    "                continue\n",
    "\n",
    "            update_full_dict_by_date(articles=articles, cur_date=cur_date)\n",
    "            update_return_dict(articles=articles, cur_date=cur_date)\n",
    "\n",
    "            print(f'Saving \"full_dict_by_date.json\" to folder \"{SELECTED_NUMBER}\"...')\n",
    "            with open(f'./output_dicts/{SELECTED_NUMBER}/full_dict_by_date.json', 'w') as f:\n",
    "                json.dump(full_dict_by_date, f, indent=2)\n",
    "            \n",
    "            print(f'Saving \"return_dict.json\" to folder \"{SELECTED_NUMBER}\"...')\n",
    "            with open(f'./output_dicts/{SELECTED_NUMBER}/return_dict.json', 'w') as f:\n",
    "                json.dump(return_dict, f, indent=2)\n",
    "\n",
    "            print(f'Saving \"full_dict_by_date.json\" to folder \"{str(cur_date_dir)}\"...') ## Saves the updated JSONs\n",
    "            with open(f'{str(cur_date_dir)}/{cur_date}.json', 'w') as f: ## Saves the whole output as today's date\n",
    "                json.dump(articles['collection'], f, indent=2)\n",
    "\n",
    "            print('='*50)\n",
    "            print('Processed date:', cur_date, 'with', len(articles['collection']), 'articles')\n",
    "            print(f'Sleeping for {delay} second(s)... Zzz...')\n",
    "            print('='*50)\n",
    "            time.sleep(delay) ## Uses the default 5 seconds if need be\n",
    "\n",
    "            num_skip += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DO_NOT_CHANGE_FLAG\n",
    "DO_NOT_CHANGE_FLAG = 10 ## Do not change this number, it is the number of splits to create and is set standard to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_OFF = 0 ## The split number to start from, if 0, then starts from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TODAY = True ## Whether or not to use today's date as the end date or to use the end date specified below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified API Call Function\n",
    "\n",
    "---\n",
    "\n",
    "The below scraper function utilizes a modified start parameter in order to begin from where the last available date for data was scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(LEFT_OFF, DO_NOT_CHANGE_FLAG):\n",
    "    global SELECTED_NUMBER\n",
    "    SELECTED_NUMBER = num ## Enter selected number here, which represents the selected number\n",
    "\n",
    "    dates = [get_last_available_date() + timedelta(days=x) for x in range((date.today() - get_last_available_date()).days + 1)] ## Gets a list of available dates\n",
    "    split_dates_list = split_dates(dates, DO_NOT_CHANGE_FLAG) ## Splits the dates into the selected number of splits\n",
    "\n",
    "    start_date = split_dates_list[SELECTED_NUMBER][0] ## Gets the start date from the selected number\n",
    "    end_date = split_dates_list[SELECTED_NUMBER][1] ## Gets the end date from the selected number\n",
    "\n",
    "    process_bioRxiv(start_date=start_date, end_date=end_date, load_dicts=False, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Combine Function\n",
    "\n",
    "---\n",
    "\n",
    "The below combine function utilizes a modified start parameter in order to begin from where the last available date for data was scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_output_dicts(path='./output_dicts/', save_path='./global_dicts/', num=0):\n",
    "    '''\n",
    "    Function to combine all of the output dicts into one JSON file by creating a placeholder, temporary dict and adding all of the values from the other dicts to it\n",
    "        This operates on the previously defined global dictionalry objects \"full_return_dict\" and \"full_return_dict_by_date\"\n",
    "        @param path: The path to the folder containing all of the output dicts\n",
    "        @return: None\n",
    "    '''\n",
    "    try:\n",
    "        with open(f'{path}/{num}/return_dict.json', 'r') as f:\n",
    "            cur_return_dict = json.load(f)\n",
    "\n",
    "        with open(f'{path}/{num}/full_dict_by_date.json', 'r') as f:\n",
    "            cur_return_dict_by_date = json.load(f)\n",
    "\n",
    "        print(f'Combining and saving \"return_dict\" license values for folder {num}...')\n",
    "        global_return_dict['license'] = {key: global_return_dict['license'].get(key, 0) + value for key, value in cur_return_dict['license'].items()} ## Adds the license counts to the global dict for each license count\n",
    "        global_return_dict['content'].update(cur_return_dict['content']) ## Adds the content to the global dict by date\n",
    "        with open(f'{save_path}/return_dict.json', 'w') as f:\n",
    "            json.dump(global_return_dict, f, indent=2)\n",
    "\n",
    "        print(f'Combining and saving \"full_dict_by_date\" content items for folder {num}...')\n",
    "        global_return_dict_by_date.update(cur_return_dict_by_date) ## Adds the content to the global return dict by date\n",
    "        with open(f'{save_path}/full_dict_by_date.json', 'w') as f:\n",
    "            json.dump(global_return_dict_by_date, f, indent=2)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f'No files in folder {num}, continuing to the next folder...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_folder():\n",
    "    '''\n",
    "    Function to create the global folder for the combined JSONs\n",
    "        @return: The path to the global folder\n",
    "    '''\n",
    "    global_folder = Path('./global_dicts/')\n",
    "    global_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return str(global_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global global_return_dict\n",
    "with open('./global_dicts/return_dict.json', 'r') as f:\n",
    "    global_return_dict = json.load(f)\n",
    "\n",
    "global global_return_dict_by_date\n",
    "with open('./global_dicts/return_dict_by_date.json', 'r') as f:\n",
    "    global_return_dict_by_date = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(DO_NOT_CHANGE_FLAG):\n",
    "    combine_output_dicts(path='./amend_dicts/',save_path=lambda _: get_global_folder(), num=num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort By License\n",
    "\n",
    "---\n",
    "\n",
    "Below we finally sort the `return_dict` based on license type and output individual `.JSON` files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
