{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Continue Scraping\n",
    "\n",
    "---\n",
    "\n",
    "This current notebook continues scraping from where it was previously left off. It is meant to be run after the initial scraping notebook, `initial-scraper.ipynb`, has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import calendar\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_available_date():\n",
    "    '''\n",
    "    Function to automatically obtain the last available date of the data and return it\n",
    "        @param None: No parameters, as this automatically obtains the last available date\n",
    "        @return last_date: The last available date of the data\n",
    "    '''\n",
    "    max_year = max([int(year_folder) for year_folder in os.listdir('./full-dict')]) ## Get last year\n",
    "    max_month = max([int(month_folder) for month_folder in os.listdir(f'./full-dict/{max_year}')]) ## Get last month\n",
    "    last_date = [x[:-5] for x in os.listdir(f'./full-dict/{max_year}/{max_month}/')][-1] ## get last date from the file\n",
    "\n",
    "    year, month, day = map(int, last_date.split('-')) # Assuming the date format is YYYY-MM-DD, split the string to get year, month, and day\n",
    "\n",
    "    return date(year, month, day) # Return a datetime.date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_return_dict(articles, cur_date):\n",
    "    '''\n",
    "    This function processes all of the articles associated with a specific date, it checks:\n",
    "        1) Whether the current DOI has ALREADY been processed, if so updates the saved JATSXML link in associated JSON\n",
    "        2) Whether the key for the license associated with the current article exists, if so, a new key is then created and added to the JSON value for that key in particular\n",
    "            @param article: The article JSON object to be processed\n",
    "            @return: None\n",
    "    '''\n",
    "    cur_date = str(cur_date)\n",
    "    articles = articles['collection']\n",
    "    for article in articles:\n",
    "        ## Tracks the number of license type count from the return_dict \"license\" object\n",
    "        return_dict['license'][article['license']] = return_dict['license'].get(article['license'], 0) + 1\n",
    "\n",
    "        ## Adds/UPDATES the most recent DOI submission from return_dict\n",
    "        return_dict['content'][article['doi']] = {\n",
    "            'date': article['date'],\n",
    "            'license': article['license'],\n",
    "            'jatsxml': article['jatsxml']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_full_dict_by_date(articles, cur_date):\n",
    "    '''\n",
    "    This function adds to a dict object where the keys are the dates in string format and the values are a list of DOIs for that date\n",
    "        @param articles: All articles in current date to extract information from\n",
    "        @param cur_date: The current date to process in YYYY-mm-dd format\n",
    "        @return: None\n",
    "    '''\n",
    "    cur_date = str(cur_date)\n",
    "    articles = articles['collection']\n",
    "    \n",
    "    full_dict_by_date[cur_date] = [{'doi': item['doi'], 'license': item['license']} for item in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dated_subfolders(date):\n",
    "    '''\n",
    "    Function to create subfolders for each of the day, month, year in the current date, specifically to store metadata from \"full_dict_by_date\"\n",
    "        @param date: The current date in YYYY-mm-dd format\n",
    "        @return: The full output path for the folder representing the current date\n",
    "    '''\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "\n",
    "    dict_main_folder = Path('./full-dict')\n",
    "\n",
    "    dict_year_folder = dict_main_folder.joinpath(str(year))\n",
    "    dict_year_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    dict_month_folder = dict_year_folder.joinpath(str(month))\n",
    "    dict_month_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return dict_month_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dates(dates, num_splits):\n",
    "    '''\n",
    "    Function that takes in a list of dates (or anything, really) and splits those dates into num_splits number of splits\n",
    "        @param dates: A list of dates (or anything really)\n",
    "        @param num_splits: The number of sub-lists to split the original list into\n",
    "    '''\n",
    "    first_last_dates_fx = lambda lis: [[li[0], li[-1]] for li in lis] ## Essentially extracts the first and last elements of the split list of dates\n",
    "    return list(first_last_dates_fx(np.array_split(dates, num_splits))) ## Returns the first and last dates of each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_folder(split_num):\n",
    "    '''\n",
    "    Function that creates a folder for the specific split applicable to the current operation\n",
    "        @param split_num: The split number to obtain the associated folder for\n",
    "        @return: The path to the folder for the split number\n",
    "    '''\n",
    "    split_main_folder = Path('./amend_dicts/')\n",
    "    split_main_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    split_folder = split_main_folder.joinpath(str(split_num))\n",
    "    split_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return split_folder ## Not needed but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bioRxiv(start_date=lambda _: date(2013, 1, 1),\n",
    "                    end_date=lambda _: date.today(),\n",
    "                    load_dicts=False,\n",
    "                    delay=5,\n",
    "                    **kwargs):\n",
    "    '''\n",
    "    Function to process the data (loading in if necessary) for a period of time defined by:\n",
    "        @param start: Beginning date in the format YYYY-mm-dd\n",
    "        @param end: End date in the format YYY-mm-dd\n",
    "        @param save_every: Saves (overrites) the last dict every n number of iterations through the data\n",
    "        @param load_dicts: Loads in the dictionary (JSON) objects associated with \"full_dict_by_date\" and \"return_dict\"\n",
    "    '''\n",
    "    global full_dict_by_date\n",
    "    full_dict_by_date = {}\n",
    "\n",
    "    global return_dict\n",
    "    return_dict = {\n",
    "        'license': {},\n",
    "        'content': {}\n",
    "    }\n",
    "\n",
    "    get_split_folder(SELECTED_NUMBER) ## Creates the folder for the current split number\n",
    "\n",
    "    if load_dicts:\n",
    "        with open(f'{SELECTED_NUMBER if SELECTED_NUMBER else \".\"}/full_dict_by_date.json', 'r') as f:\n",
    "            full_dict_by_date = json.load(f)\n",
    "        with open(f'{SELECTED_NUMBER if SELECTED_NUMBER else \".\"}/return_dict.json', 'r') as f:\n",
    "            return_dict = json.load(f)\n",
    "\n",
    "        start_date = date(full_dict_by_date.keys().sort()[-1]) + timedelta(days=1) ## Gets the last available date and adds 1 IF not the first date, otherwise starts at Jan 1, 2013\n",
    "\n",
    "    dates = [start_date + timedelta(days=x) for x in range((end_date  - start_date).days + 1)] ## Gets a list of available dates\n",
    "\n",
    "    for cur_date in dates:\n",
    "\n",
    "        flag = True ## Flag of whether or not to continue the loop to get more pages\n",
    "        num_skip = 0 ## Flag to skip as per API instructions\n",
    "        while flag:\n",
    "            cur_date_dir = get_dated_subfolders(cur_date)\n",
    "\n",
    "            url = f'https://api.biorxiv.org/details/biorxiv/{str(cur_date)}/{str(cur_date)}/{num_skip}'\n",
    "\n",
    "            response = requests.get(url) ## Processes request and waits for response / articles in JSON format\n",
    "            response.raise_for_status()\n",
    "            articles = response.json()\n",
    "\n",
    "            if articles['messages'][0]['status'] != 'ok': ## Continues onto the next date if posts for the current date are unavailable\n",
    "                flag = False\n",
    "                continue\n",
    "\n",
    "            update_full_dict_by_date(articles=articles, cur_date=cur_date)\n",
    "            update_return_dict(articles=articles, cur_date=cur_date)\n",
    "\n",
    "            print(f'Saving \"full_dict_by_date.json\" to folder \"{SELECTED_NUMBER}\"...')\n",
    "            with open(f'./amend_dicts/{SELECTED_NUMBER}/full_dict_by_date.json', 'w') as f:\n",
    "                json.dump(full_dict_by_date, f, indent=2)\n",
    "            \n",
    "            print(f'Saving \"return_dict.json\" to folder \"{SELECTED_NUMBER}\"...')\n",
    "            with open(f'./amend_dicts/{SELECTED_NUMBER}/return_dict.json', 'w') as f:\n",
    "                json.dump(return_dict, f, indent=2)\n",
    "\n",
    "            print(f'Saving \"full_dict_by_date.json\" to folder \"{str(cur_date_dir)}\"...') ## Saves the updated JSONs\n",
    "            with open(f'{str(cur_date_dir)}/{cur_date}.json', 'w') as f: ## Saves the whole output as today's date\n",
    "                json.dump(articles['collection'], f, indent=2)\n",
    "\n",
    "            print('='*50)\n",
    "            print('Processed date:', cur_date, 'with', len(articles['collection']), 'articles')\n",
    "            print(f'Sleeping for {delay} second(s)... Zzz...')\n",
    "            print('='*50)\n",
    "            time.sleep(delay) ## Uses the default 5 seconds if need be\n",
    "\n",
    "            num_skip += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_folder(path):\n",
    "    '''\n",
    "    Function that gets the last folder in a given path\n",
    "        @param path: The path to get the last folder from\n",
    "        @return: The last folder in the path\n",
    "    '''\n",
    "    last_directory = None\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        if dirnames:\n",
    "            last_directory = sorted(dirnames)[-1]\n",
    "    if last_directory:\n",
    "        return last_directory\n",
    "    return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DO_NOT_CHANGE_FLAG\n",
    "DO_NOT_CHANGE_FLAG = 10 ## Do not change this number, it is the number of splits to create and is set standard, the number of splits is iteratively changed for optimal performance (to create splits that are divisible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_folder_num = int(get_last_folder('./amend_dicts/')) ## Gets the last folder number\n",
    "SELECTED_NUMBER = last_folder_num + 1 ## Gets the last folder number and adds 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified API Call Function\n",
    "\n",
    "---\n",
    "\n",
    "The below scraper function utilizes a modified start parameter in order to begin from where the last available date for data was scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process_bioRxiv(start=SELECTED_NUMBER, flag=DO_NOT_CHANGE_FLAG):\n",
    "    '''\n",
    "    Function to run the process_bioRxiv function for a specific number of splits defined previously.\n",
    "    '''\n",
    "    for num in range(start, start + flag):\n",
    "        dates = [get_last_available_date() + timedelta(days=x) for x in range((date.today() - get_last_available_date()).days + 1)] \n",
    "\n",
    "        try:\n",
    "            split_dates_list = split_dates(dates, flag)\n",
    "            \n",
    "            start_date = split_dates_list[num][0]\n",
    "            end_date = split_dates_list[num][1]\n",
    "\n",
    "            process_bioRxiv(start_date=start_date, end_date=end_date, load_dicts=False, delay=1)\n",
    "        except IndexError as e: # or a more specific exception\n",
    "            print(f\"Error occured for split {num}: {e}\")\n",
    "            flag -= 1\n",
    "            if flag == 0:\n",
    "                print('No more splits to try, exiting...')\n",
    "                break\n",
    "            print(f'Skipping to next split and trying with flag {flag}')\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving \"full_dict_by_date.json\" to folder \"4\"...\n",
      "Saving \"return_dict.json\" to folder \"4\"...\n",
      "Saving \"full_dict_by_date.json\" to folder \"full-dict\\2023\\10\"...\n",
      "==================================================\n",
      "Processed date: 2023-10-12 with 100 articles\n",
      "Sleeping for 1 second(s)... Zzz...\n",
      "==================================================\n",
      "Saving \"full_dict_by_date.json\" to folder \"4\"...\n",
      "Saving \"return_dict.json\" to folder \"4\"...\n",
      "Saving \"full_dict_by_date.json\" to folder \"full-dict\\2023\\10\"...\n",
      "==================================================\n",
      "Processed date: 2023-10-12 with 36 articles\n",
      "Sleeping for 1 second(s)... Zzz...\n",
      "==================================================\n",
      "Error occured for split 5: index 0 is out of bounds for axis 0 with size 0\n",
      "Skipping to next split and trying with flag 9\n",
      "Error occured for split 6: index 0 is out of bounds for axis 0 with size 0\n",
      "Skipping to next split and trying with flag 8\n",
      "Error occured for split 7: index 0 is out of bounds for axis 0 with size 0\n",
      "Skipping to next split and trying with flag 7\n",
      "Error occured for split 8: index 0 is out of bounds for axis 0 with size 0\n",
      "Skipping to next split and trying with flag 6\n",
      "Error occured for split 9: list index out of range\n",
      "Skipping to next split and trying with flag 5\n",
      "Error occured for split 10: list index out of range\n",
      "Skipping to next split and trying with flag 4\n",
      "Error occured for split 11: list index out of range\n",
      "Skipping to next split and trying with flag 3\n",
      "Error occured for split 12: list index out of range\n",
      "Skipping to next split and trying with flag 2\n",
      "Error occured for split 13: list index out of range\n",
      "Skipping to next split and trying with flag 1\n"
     ]
    }
   ],
   "source": [
    "run_process_bioRxiv() ## Runs the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Combine Function\n",
    "\n",
    "---\n",
    "\n",
    "The below combine function utilizes a modified start parameter in order to begin from where the last available date for data was scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_output_dicts(path='./output_dicts/', save_path='./global_dicts/', num=0):\n",
    "    '''\n",
    "    Function to combine all of the output dicts into one JSON file by creating a placeholder, temporary dict and adding all of the values from the other dicts to it\n",
    "        This operates on the previously defined global dictionalry objects \"full_return_dict\" and \"full_return_dict_by_date\"\n",
    "        @param path: The path to the folder containing all of the output dicts\n",
    "        @return: None\n",
    "    '''\n",
    "    try:\n",
    "        with open(f'{path}/{num}/return_dict.json', 'r') as f:\n",
    "            cur_return_dict = json.load(f)\n",
    "\n",
    "        with open(f'{path}/{num}/full_dict_by_date.json', 'r') as f:\n",
    "            cur_return_dict_by_date = json.load(f)\n",
    "\n",
    "        print(f'Combining and saving \"return_dict\" license values for folder {num}...')\n",
    "        global_return_dict['license'] = {key: global_return_dict['license'].get(key, 0) + value for key, value in cur_return_dict['license'].items()} ## Adds the license counts to the global dict for each license count\n",
    "        global_return_dict['content'].update(cur_return_dict['content']) ## Adds the content to the global dict by date\n",
    "        with open(f'{save_path}/return_dict.json', 'w') as f:\n",
    "            json.dump(global_return_dict, f, indent=2)\n",
    "\n",
    "        print(f'Combining and saving \"full_dict_by_date\" content items for folder {num}...')\n",
    "        global_return_dict_by_date.update(cur_return_dict_by_date) ## Adds the content to the global return dict by date\n",
    "        with open(f'{save_path}/full_dict_by_date.json', 'w') as f:\n",
    "            json.dump(global_return_dict_by_date, f, indent=2)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f'No files in folder {num}, continuing to the next folder...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_folder():\n",
    "    '''\n",
    "    Function to create the global folder for the combined JSONs\n",
    "        @return: The path to the global folder\n",
    "    '''\n",
    "    global_folder = Path('./global_dicts/')\n",
    "    global_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    return str(global_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "global global_return_dict\n",
    "with open('./global_dicts/return_dict.json', 'r') as f:\n",
    "    global_return_dict = json.load(f)\n",
    "\n",
    "global global_return_dict_by_date\n",
    "with open('./global_dicts/full_dict_by_date.json', 'r') as f:\n",
    "    global_return_dict_by_date = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining and saving \"return_dict\" license values for folder 0...\n",
      "Combining and saving \"full_dict_by_date\" content items for folder 0...\n",
      "Combining and saving \"return_dict\" license values for folder 1...\n",
      "Combining and saving \"full_dict_by_date\" content items for folder 1...\n",
      "Combining and saving \"return_dict\" license values for folder 2...\n",
      "Combining and saving \"full_dict_by_date\" content items for folder 2...\n",
      "Combining and saving \"return_dict\" license values for folder 3...\n",
      "Combining and saving \"full_dict_by_date\" content items for folder 3...\n",
      "Combining and saving \"return_dict\" license values for folder 4...\n",
      "Combining and saving \"full_dict_by_date\" content items for folder 4...\n",
      "No files in folder 5, continuing to the next folder...\n",
      "No files in folder 6, continuing to the next folder...\n",
      "No files in folder 7, continuing to the next folder...\n",
      "No files in folder 8, continuing to the next folder...\n",
      "No files in folder 9, continuing to the next folder...\n"
     ]
    }
   ],
   "source": [
    "for num in range(DO_NOT_CHANGE_FLAG):\n",
    "    combine_output_dicts(path='./amend_dicts/',save_path=get_global_folder(), num=num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort By License\n",
    "\n",
    "---\n",
    "\n",
    "Below we finally sort the `return_dict` based on license type and output individual `.JSON` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "global global_dict_by_license\n",
    "global_dict_by_license = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dict_by_license = {\n",
    "    k1: {\n",
    "        k2: {'date': v2['date'], 'jatsxml': v2['jatsxml']}\n",
    "        for k2, v2 in global_return_dict['content'].items() if v2['license'] == k1\n",
    "    }\n",
    "    for k1 in global_return_dict['license']\n",
    "}\n",
    "\n",
    "with open('./global_dicts/dict_by_license.json', 'w') as f:\n",
    "    json.dump(global_dict_by_license, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
